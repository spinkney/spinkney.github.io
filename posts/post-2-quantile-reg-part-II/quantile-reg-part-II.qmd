---
title: "Quantile Regressions in Stan: Part II"
author: "Sean Pinkney"
date: "10/18/2022"
categories: [stan, quantile]
bibliography: references.bib
draft: false
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r, message=FALSE, echo=FALSE}
library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
register_knitr_engine(override = FALSE)
```

In [part I](/posts/post-2-quantile-reg-part-I/quantile-reg.qmd) I showed the basic Bayesian quantile regression using the asymmetric Laplace distribution and augmented scheme. In this post I'm going to show an approximate likelihood, score method proposed in @score.

# First, why not use the asymmetric Laplace?

The ALD (asymmetric Laplace distribution) is great. It's fast and gives point results comparable to the frequentist estimator. However, @yang2016 showed that the asymptotic variance of the posterior distribution is "incorrect". It does not correspond to the frequentist interval. A reason is that the ALD and pinball loss function, $\rho_\tau(u) = \frac{|u| + (2\tau - 1)u}{2}$, is maximized at the standard frequentist quantile but does not correspond to a true data generating likelihood. In the aforementioned paper, they propose a correction to the interval after sampling. I might show this in another post but post-processing is annoying. The score method asymptotically approaches the correct estimates with the correct interval.

# Quantile regression with score

The score method includes the regressors into the objective function as $$
s_\tau(\beta) = \sum_{i=1}^{n} x_i \psi_\tau(y_i - x_i^{\top}\beta).
$$ where $\psi_\tau(u) = \tau - I(u < 0)$.

::: callout-warning
Warning! It seems that $\psi$ doesn't work for me! What does work is using $\rho_\tau = \frac{|u| + (2\tau - 1)u}{2}$ as before. They mention that the loss is the same but then use new notation indicating that it's different. Anyway, this may be what they intended.
:::

@score consider the following "working" likelihood $$
\mathcal{L}(y \mid x,\, \beta) = C \exp\bigg(-\frac{1}{2n} s_\tau(\beta)^{\top} W s_\tau(\beta)  \bigg),
$$ where $W$ is a $p \times p$ positive definite weight matrix, and C is a constant free of $\beta$. The quadratic form and the given objective function implies that the quantile estimator is maximized at the typical quantile estimate.

They then choose a weight matrix which gives the correct posterior covariance as

$$
W = \frac{n}{\tau (1 - \tau)}\bigg(\sum_{i=1}^n x_i x_i^{\top} \bigg).
$$ In fact, they also show how this can incorporate multiple $\tau$'s (I'll show this in part III) which allows more flexible modeling options, such as a squared exponential kernel that shares information across $\beta$ quantile estimates which are near each other. The $W$ matrix for multiple quantiles is given as

$$
W = (Q \otimes G )^{-1}, \text{ where } Q = (\min(\tau_i, \tau_j) - \tau_i \tau_j)_{ij}, \; G = \frac{1}{n}\sum_{i=1}^n x_i x_i^{\top}.
$$ A nice property of $W$ is that it depends all on user input data so can be constructed outside of the MCMC iterations.

## Score function

The following Stan model implements the single $\tau$ code. I'll simulate some data and compare to the asymmetric Laplace:

```{cmdstan, eval=TRUE, output.var="score_qr", class.source = 'fold-show'}
functions{
  vector q_loss(real q, vector u){
    return 0.5 * (abs(u) + (2 * q - 1) * u);
  }
    
  vector score(real q, vector y, matrix x, vector beta) {
    return transpose(x) * q_loss(q, y - x * beta);
  }
    
  matrix make_w (matrix x, real q) {
    int N = rows(x);
    int P = cols(x);
    real alpha = 1 / (q * (1 - q));
    matrix[P, P] x_out = inverse_spd(crossprod(x));

    return alpha * x_out * N;
  }
}
data {
  int N;                 // Number of observation
  int P;                 // Number of predictors
  real q;
  vector[N] y;           // Response variable sorted
  matrix[N, P] x;
}
transformed data {
  matrix[P, P] W = make_w(x, q);
}
parameters {
  vector[P] beta;
}
model {
  vector[P] score_vec = score(q, y, x, beta);

  beta ~ normal(0, 4);

  target += -quad_form(W, score_vec) / (2 * N);
}
```

```{cmdstan, echo=FALSE, eval=TRUE, output.var="asym_laplace"}
functions{
real q_loss(real q, vector u){
  return 0.5 * sum(abs(u) + (2 * q - 1) * u);
}

real ald_lpdf(vector y, real q, real sigma, vector q_est){
  int N = num_elements(y);
  
  return N * (log(q) + log1m(q) - log(sigma)) - q_loss(q, y - q_est) / sigma;
}
}
data {
  int N;                   // Number of observation
  int P;                   // Number of predictors
  real<lower=0, upper=1> q;
  vector[N] y;             // Response variable sorted
  matrix[N, P] x;
}
parameters {
  vector[P] beta;
  real<lower=0> sigma;
}
model {
  beta ~ normal(0, 4);
  sigma ~ exponential(1);
  y ~ ald(q, sigma, x * beta);
}
```

Let's simulate some data and see how well it performs.

::: callout-note
I have to dial up the iterations a bit for the score method as it's less efficient from an ESS perspective.
:::

```{r, message=FALSE, warning=FALSE, class.source = 'foldable'}
set.seed(12312)
library(quantreg)
N     <- 1000
x     <- runif(N, max=10)
alpha <- -1
beta  <- 2
y     <- alpha + beta * x + rnorm(N, sd = .5 * x)
q     <- 0.05

# frequentist estimate
out_freq <- quantreg::rq(y ~ x, tau = q)

out_score <- score_qr$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)

out_asym_laplace <- asym_laplace$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)

print(summary(out_freq, se = "boot"))
out_score$summary()
out_asym_laplace$summary("beta")
```

### Modified Score

We see that the frequentist estimate from `quantreg` and the asymmetric Laplace outputs are similar. The score method under-estimates the intercept where the credible interval doesn't even include -1 and the coefficient for $x$ seems to be over-estimated. Noticing this behavior, let's see what modifying the weight matrix for the intercept does. We'd like the estimate to correspond to shrinking standard deviations with increasing $N$. Something like $\log(N)$ will work. The modification being

``` stan
  matrix make_w (matrix x, real q) {
    int N = rows(x);
    int P = cols(x);
    real alpha = 1 / (q * (1 - q));
    matrix[P, P] x_out = crossprod(x);

    x_out[2:P, 1] /= 1 + log(N);
    x_out[1, 2:P] = transpose(x_out[2:P, 1]);
  
    return alpha * inverse_spd(x_out) * N;
  }
```

Let's re-estimate the above with the modification.

```{cmdstan, echo=FALSE, eval=TRUE, output.var="score_qr_modified", class.source = 'foldable'}
functions{
  vector q_loss(real q, vector u){
    return 0.5 * (abs(u) + (2 * q - 1) * u);
  }
    
  vector score(real q, vector y, matrix x, vector beta) {
    return transpose(x) * q_loss(q, y - x * beta);
  }
    
  
  matrix make_w (matrix x, real q) {
    int N = rows(x);
    int P = cols(x);
    real alpha = 1 / (q * (1 - q));
    matrix[P, P] x_out = crossprod(x);

    x_out[2:P, 1] /= 1 + log(N);
    x_out[1, 2:P] = transpose(x_out[2:P, 1]);
  
    return alpha * inverse_spd(x_out) * N;
  }
}
data {
  int N;                 // Number of observation
  int P;                 // Number of predictors
  real q;
  vector[N] y;           // Response variable sorted
  matrix[N, P] x;
}
transformed data {
  matrix[P, P] W = make_w(x, q);
}
parameters {
  vector[P] beta;
}
model {
  vector[P] score_vec = score(q, y, x, beta);

  beta ~ normal(0, 4);

  target += -quad_form(W, score_vec) / (2 * N);
}
```

```{r}
out_score_modified <- score_qr_modified$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)
out_score_modified$summary()
```

We now have estimates which line up a bit closer with the frequentist estimate but is completely unprincipled. Out of curiosity let's see how this compares on simulated data with $N = 10000$

```{r, message=FALSE, echo=FALSE}
set.seed(12312)
N     <- 10000
x     <- runif(N, max=10)
alpha <- -1
beta  <- 2
y     <- alpha + beta * x + rnorm(N, sd = .5 * x)
q     <- 0.05

out_score <- score_qr$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)

out_score_modified <- score_qr_modified$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)

out_asym_laplace <- asym_laplace$sample(
  data = list(N = N,
              P = 2,
              q = q,
              y = y,
              x = as.matrix(data.frame(alpha = 1, x = x))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)
```

### Score

```{r}
out_score$summary()
```

### Modified Score

```{r}
out_score_modified$summary()
```

### Quantreg

```{r}
print(summary(out_freq, se = "boot"))
```

### Asymmetric Laplace

```{r}
out_asym_laplace$summary("beta")
```

It does appear to be better. Let's see how this does on the ImmunogG data.

# Test on ImmunogG data

```{r}
library(Brq)
data("ImmunogG")
dat <- data.frame(y = ImmunogG$IgG, 
                  alpha = 1, 
                  x = ImmunogG$Age, 
                  xsq = ImmunogG$Age^2)

out_score <- score_qr$sample(
  data = list(N = nrow(ImmunogG),
              P = 3,
              q = q,
              y = ImmunogG$IgG,
              x = as.matrix(dat[, 2:4])),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)

out_score_mod <- score_qr_modified$sample(
  data = list(N = nrow(ImmunogG),
              P = 3,
              q = q,
              y = ImmunogG$IgG,
              x = as.matrix(dat[, 2:4])),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 1000,
  iter_sampling = 1000,
    refresh = 0,
  show_messages = FALSE
)

out_al <- asym_laplace$sample(
  data = list(N = nrow(ImmunogG),
              P = 3,
              q = q,
              y = ImmunogG$IgG,
              x = as.matrix(dat[, 2:4])),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)

out_freq <- rq(y ~ x + xsq, 
               data = dat,
               tau = q)
```

### Results

### Orginal score:

```{r}
out_score$summary()
```

### Modified score:

```{r}
out_score_mod$summary()
```

### Quantreg:

```{r}
summary(out_freq, se = "boot")
```

### Asymmetric Laplace:

```{r}
out_al$summary()
```

Interestingly, the results of the score method lie between the frequentist and asymmetric Laplace methods.

# Conclusion

The score method is promising but it doesn't seem like one would use the asymmetric Laplace due to the ad-hoc nature of the intercept. One thing the score method has is incorporating multipled quantiles in a principled way that allows you to put priors on the estimates that correspond to linking them. It also doesn't need to re-run the data through each quantile. In the next post, part III, I'll show this. If you have a bunch of different quantiles, lots of data, and want to understand the correlations across parameters then the score method may be right for you.

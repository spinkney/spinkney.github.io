---
title: "Quantile Regressions in Stan: Part I"
author: "Sean Pinkney"
date: "10/15/2022"
categories: [stan, quantile]
bibliography: references.bib
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r, message=FALSE, echo=FALSE}
library(cmdstanr)
check_cmdstan_toolchain(fix = TRUE, quiet = TRUE)
register_knitr_engine(override = FALSE)
```


Quantile regressions are relatively new in the Bayesian literature first appearing in @yu who described an asymmetric Laplace distribution to estimate the conditional quantiles. This post is going to show the asymmetric Laplace and the augmented version. In part II I will show the score likelihood method.

The asymmetric Laplace distribution is given as 

$$
\operatorname{ALD}(Y \mid x, \beta) = \frac{\tau^n (1 - \tau)^n}{\sigma^n}\exp \bigg( -\frac{\sum_{i=1}^n \rho_\tau (y_i - x_i^{\top}\beta)}{\sigma} \bigg)
$$

where

$$
\rho_\tau(u) = \frac{|u| + (2\tau - 1)u}{2}
$$

We may write this in Stan as


```{cmdstan, output.var="asymetric_laplace"}
functions{
real q_loss(real q, vector u){
  return 0.5 * sum(abs(u) + (2 * q - 1) * u);
}

real ald_lpdf(vector y, real q, real sigma, vector q_est){
  int N = num_elements(y);
  
  return N * (log(q) + log1m(q) - log(sigma)) - q_loss(q, y - q_est) / sigma;
}
}
data {
  int N;                   // Number of observation
  int P;                   // Number of predictors
  real<lower=0, upper=1> q;
  vector[N] y;             // Response variable sorted
  matrix[N, P] x;
}
parameters {
  vector[P] beta;
  real<lower=0> sigma;
}
model {
  beta ~ normal(0, 4);
  sigma ~ exponential(1);
  y ~ ald(q, sigma, x * beta);
}
```


Let's compare the 5th percentile to the `Brq` R package


```{r, message=F}
library(Brq)
data("ImmunogG")
out_asym_laplace <- asymetric_laplace$sample(
  data = list(N = nrow(ImmunogG),
              P = 3,
              q = 0.05,
              y = ImmunogG$IgG,
              x = as.matrix(data.frame(alpha = 1, x = ImmunogG$Age, xsq = ImmunogG$Age^2))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)
out_asym_laplace$summary()

y = ImmunogG$IgG
x = ImmunogG$Age
X=cbind(x, x^2)
fit = Brq(y ~ X , tau= 0.05,runs= 2000, burn=1000)
print(summary(fit))
```


### Using the covariates for extra information

In @Lee2022 they detail a Bayesian approach to the envelope quantile regression. In all honesty, I briefly skimmed the paper and gathered that by adding a regression on the covariates and using this information in the asymmetric Laplace can help identification. This isn't a recreation of their method but a quick detour to see if a simplified version of what they do increases ESS.

The idea is that we use the variability in the covariance of the regressors to help identify the coefficients on the ALD regression. I now have to separate out the intercept into $\alpha$.


```{cmdstan, output.var="asymetric_laplace_envelope"}
functions{
real q_loss(real q, vector u){
  return 0.5 * sum(abs(u) + (2 * q - 1) * u);
}

real ald_lpdf(vector y, real q, real sigma, vector q_est){
  int N = num_elements(y);
  
  return N * (log(q) + log1m(q) - log(sigma)) - q_loss(q, y - q_est) / sigma;
}
}
data {
  int N;                   // Number of observation
  int P;                   // Number of predictors
  real<lower=0, upper=1> q;
  vector[N] y;             // Response variable sorted
  matrix[N, P] x;
}
transformed data {
  array[N] vector[P] X;
  
  for (i in 1:P) {
    X[:, i] = to_array_1d(x[:, i]);
  }
}
parameters {
  vector[P] eta;
  real<lower=0> sigma;
  vector[P] mu;
  cholesky_factor_corr[P] L;
  vector<lower=0>[P] gamma;
  real alpha;
}
transformed parameters {
  vector[P] beta = eta .* gamma;
}
model {
  eta ~ normal(0, 4);
  sigma ~ exponential(1);
  y ~ ald(q, sigma, alpha + x * beta);
  alpha ~ normal(0, 4);
  
  gamma ~ exponential(1);
  mu ~ normal(0, 4);

  X ~ multi_normal_cholesky(mu, diag_pre_multiply(gamma, L));
}
```


Let's fit the model. 

```{r}
out_envelope <- asymetric_laplace_envelope$sample(
  data = list(N = nrow(ImmunogG),
              P = 2,
              q = 0.05,
              y = ImmunogG$IgG,
              x = as.matrix(data.frame(x = ImmunogG$Age, xsq = ImmunogG$Age^2))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)

out_envelope$summary(c("alpha", "beta", "sigma"))
```

Slightly different estimates, higher ESS, slower to fit, but lower standard deviations! Neat.

## Scale-mixture representation

The above may also be written as a mixture of exponential and normal distributions. Letting

$$
\begin{aligned}
z_i &\sim \operatorname{Exp}(1) \\
\sigma &\sim \operatorname{Exp}(1) \\
y_i &\sim \mathcal{N}(x_i^{\top}\beta_p + \sigma \theta z_i,\, \tau \sigma \sqrt{z_i})
\end{aligned}
$$ where

$$
\begin{aligned}
\theta &= \frac{1 - 2 q}{q (1 - q)} \\
\tau &= \sqrt{\frac{2}{q(1 -q)}}
\end{aligned}
$$

The following Stan model implements the augmented approach.


```{cmdstan, eval=TRUE, output.var="augmented_laplace"}
data {
  int N;                   // Number of observation
  int P;                   // Number of predictors
  real<lower=0, upper=1> q;
  vector[N] y;             // Response variable sorted
  matrix[N, P] x;
}
transformed data {
  real theta = (1 - 2 * q) / (q * (1 - q));
  real tau = sqrt(2 / (q * (1 - q)));
}
parameters {
  vector[P] beta;
  vector<lower=0>[N] z;
  real<lower=0> sigma;
}
model {
  beta ~ normal(0, 2);
  sigma ~ exponential(1);
  
  // Data Augmentation
  z ~ exponential(1);
  y ~ normal(x * beta + sigma * theta * z, tau * sqrt(z) * sigma);
}
```


Let's fit the model


```{r}
out_aug_laplace <- augmented_laplace$sample(
  data = list(N = nrow(ImmunogG),
              P = 3,
              q = 0.05,
              y = ImmunogG$IgG,
              x = as.matrix(data.frame(alpha = 1, x = ImmunogG$Age, xsq = ImmunogG$Age^2))),
  seed = 12123123, 
  parallel_chains = 4,
  iter_warmup = 500,
  iter_sampling = 500,
    refresh = 0,
  show_messages = FALSE
)
out_aug_laplace$summary("beta")
```


Ouch! Divergences. Although the parameter estimates look comparable. Let's inspect the pairs plots between the augmented version and the asymmetric Laplace.


```{r}
library(bayesplot)
np_aug_laplace <- nuts_params(out_aug_laplace)
np_asym_laplace <- nuts_params(out_asym_laplace)
mcmc_pairs(out_aug_laplace$draws(c("beta", "sigma")), np = np_aug_laplace)
mcmc_pairs(out_asym_laplace$draws(c("beta", "sigma")), np = np_asym_laplace)
```


The pairs plots don't appear to show the divergence originating be due to `beta` or `sigma`. It's most likely the augmentation variables `z`. As I'm up for time on this, I'd be curious to test out alternative specifications or see if anyone has encountered these issues with the augmented version and found a solution.

For the next post, we'll take a look at an approximate likelihood using the data which asymptotically approaches the true likelihood with more data.





{
  "hash": "a50304c1cdea3c238609c59396c1efb3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Quantile Regressions in Stan: Part II\"\nauthor: \"Sean Pinkney\"\ndate: \"10/18/2022\"\ncategories: [stan, quantile]\nbibliography: references.bib\ndraft: false\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n::: {.cell}\n\n:::\n\n\n[Part I](/posts/post-2-quantile-reg-series/post-2-quantile-reg-part-I/quantile-reg.html) showed the basic Bayesian quantile regression using the asymmetric Laplace distribution and augmented scheme. In this post I'm going to show an approximate likelihood, score method proposed in @score.\n\n# First, why not use the asymmetric Laplace?\n\nThe ALD (asymmetric Laplace distribution) is great. It's fast and gives point results comparable to the frequentist estimator. However, @yang2016 showed that the asymptotic variance of the posterior distribution is \"incorrect\". It does not correspond to the frequentist interval. A reason is that the ALD and pinball loss function, $\\rho_\\tau(u) = \\frac{|u| + (2\\tau - 1)u}{2}$, is maximized at the standard frequentist quantile but does not correspond to a true data generating likelihood. In the aforementioned paper, they propose a correction to the interval after sampling. I might show this in another post but post-processing is annoying. The score method asymptotically approaches the correct estimates with the correct interval.\n\n# Quantile regression with score\n\nThe score method includes the regressors into the objective function as $$\ns_\\tau(\\beta) = \\sum_{i=1}^{n} x_i \\psi_\\tau(y_i - x_i^{\\top}\\beta).\n$$ where $\\psi_\\tau(u) = \\tau - I(u < 0)$.\n\n::: callout-warning\nWarning! It seems that $\\psi$ doesn't work for me! What does work is using $\\rho_\\tau = \\frac{|u| + (2\\tau - 1)u}{2}$ as before. They mention that the loss is the same but then use new notation indicating that it's different. Anyway, this may be what they intended.\n:::\n\n@score consider the following \"working\" likelihood $$\n\\mathcal{L}(y \\mid x,\\, \\beta) = C \\exp\\bigg(-\\frac{1}{2n} s_\\tau(\\beta)^{\\top} W s_\\tau(\\beta)  \\bigg),\n$$ where $W$ is a $p \\times p$ positive definite weight matrix, and C is a constant free of $\\beta$. The quadratic form and the given objective function implies that the quantile estimator is maximized at the typical quantile estimate.\n\nThey then choose a weight matrix which gives the correct posterior covariance as\n\n$$\nW = \\frac{n}{\\tau (1 - \\tau)}\\bigg(\\sum_{i=1}^n x_i x_i^{\\top} \\bigg).\n$$ In fact, they also show how this can incorporate multiple $\\tau$'s (I'll show this in part III) which allows more flexible modeling options, such as a squared exponential kernel that shares information across $\\beta$ quantile estimates which are near each other. The $W$ matrix for multiple quantiles is given as\n\n$$\nW = (Q \\otimes G )^{-1}, \\text{ where } Q = (\\min(\\tau_i, \\tau_j) - \\tau_i \\tau_j)_{ij}, \\; G = \\frac{1}{n}\\sum_{i=1}^n x_i x_i^{\\top}.\n$$ A nice property of $W$ is that it depends all on user input data so can be constructed outside of the MCMC iterations.\n\n## Score function\n\nThe following Stan model implements the single $\\tau$ code. I'll simulate some data and compare to the asymmetric Laplace:\n\n\n::: {.cell output.var='score_qr'}\n\n```{.stan .fold-show .cell-code}\nfunctions{\n  vector q_loss(real q, vector u){\n    return 0.5 * (abs(u) + (2 * q - 1) * u);\n  }\n    \n  vector score(real q, vector y, matrix x, vector beta) {\n    return transpose(x) * q_loss(q, y - x * beta );\n  }\n    \n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_out = inverse_spd(crossprod(x));\n\n    return alpha * x_out * N;\n  }\n}\ndata {\n  int N;                 // Number of observation\n  int P;                 // Number of predictors\n  real q;\n  vector[N] y;           // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  matrix[P, P] W = make_w(x, q);\n}\nparameters {\n  vector[P] beta;\n}\nmodel {\n  vector[P] score_vec = score(q, y, x, beta);\n\n  beta ~ normal(0, 4);\n\n  target += -quad_form(W, score_vec) / (2 * N);\n}\n```\n:::\n\n\n\n::: {.cell output.var='asym_laplace'}\n\n:::\n\n\nLet's simulate some data and see how well it performs.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nset.seed(12312)\nlibrary(quantreg)\nN     <- 1000\nx     <- runif(N, max=10)\nalpha <- -1\nbeta  <- 2\ny     <- alpha + beta * x + rnorm(N, sd = .5 * x)\nq     <- 0.05\n\n# frequentist estimate\nout_freq <- quantreg::rq(y ~ x, tau = q)\n\nout_score <- score_qr$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\nout_asym_laplace <- asym_laplace$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n```\n:::\n\n\n### Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -832.   -831.   1.00   0.671  -834.   -831.    1.01     452.     568.\n2 beta[1]    -1.41   -1.41 0.171  0.181    -1.68   -1.12  1.02     393.     486.\n3 beta[2]     1.30    1.30 0.0271 0.0288    1.25    1.34  1.02     384.     468.\n```\n\n\n:::\n:::\n\n\n### Asymmetric Laplace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_asym_laplace$summary(c(\"beta\", \"sigma\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  variable   mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  -1.08  -1.07  0.0855  0.0882  -1.23  -0.961  1.00     747.     793.\n2 beta[2]   1.24   1.24  0.0187  0.0193   1.22   1.28   1.00     751.     945.\n3 sigma     0.248  0.248 0.00789 0.00797  0.236  0.261  1.00    1087.    1009.\n```\n\n\n:::\n:::\n\n\n### Quantreg\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(out_freq, se = \"boot\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: quantreg::rq(formula = y ~ x, tau = q)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept) -1.03935  0.12013   -8.65201  0.00000\nx            1.23539  0.03651   33.84091  0.00000\n```\n\n\n:::\n:::\n\n\n### Modified Score\n\nWe see that the frequentist estimate from `quantreg` and the asymmetric Laplace outputs are similar. The score method under-estimates the intercept where the credible interval doesn't even include -1 and the coefficient for $x$ seems to be over-estimated. Noticing this behavior, let's see about adding `sigma` the standard deviation to the estimation.\n\nLet's re-estimate the above with the modification.\n\n\n::: {.cell output.var='score_qr_modified'}\n\n```{.stan .foldable .cell-code}\nfunctions{\n  vector q_loss(real q, vector u){\n    return 0.5 * (abs(u) + (2 * q - 1) * u);\n  }\n    \n  vector score(real q, vector y, matrix x, vector beta) {\n    return transpose(x) * q_loss(q, y - x * beta);\n  }\n\n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_out = crossprod(x);\n\n   return alpha * inverse_spd(x_out) * N;\n\n  }\n}\ndata {\n  int N;                 // Number of observation\n  int P;                 // Number of predictors\n  real q;\n  vector[N] y;           // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  matrix[P, P] W = make_w(x, q);\n}\nparameters {\n  vector[P] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  vector[P] score_vec = score( q, y, x, beta ) / sigma ;\n\n  beta ~ normal(0, 4);\n\n  target += -quad_form(W, score_vec) / (2 * N) - N * log(sigma);\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_modified <- score_qr_modified$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\nout_score_modified$summary() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 10\n  variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -755.   -755.   1.22   1.01   -757.   -754.    1.00     751.     946.\n2 beta[1]    -1.41   -1.40 0.206  0.200    -1.76   -1.06  1.00     544.     841.\n3 beta[2]     1.30    1.30 0.0322 0.0319    1.25    1.35  1.00     551.     838.\n4 sigma       1.29    1.29 0.0293 0.0293    1.24    1.34  1.00    1007.    1080.\n```\n\n\n:::\n:::\n\n\nThe estimates are basically the same but the standard errors increased. We'll see that the increased in standard errors makes more reasonable posterior intervals.\n\n### Simulated data with N = 10000\n\n\n::: {.cell}\n\n:::\n\n\n### Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  <chr>        <dbl>     <dbl>   <dbl>   <dbl>     <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -9621.    -9620.    0.938   0.760   -9623.    -9.62e+3  1.00     852.\n2 beta[1]     -0.913    -0.911 0.0339  0.0330     -0.972 -8.60e-1  1.01     545.\n3 beta[2]      1.15      1.15  0.00633 0.00615     1.14   1.16e+0  1.01     617.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n### Modified Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_modified$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 10\n  variable      mean    median      sd     mad       q5       q95  rhat ess_bulk\n  <chr>        <dbl>     <dbl>   <dbl>   <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__     -8273.    -8273.    1.21    1.01    -8275.   -8272.     1.00     949.\n2 beta[1]     -0.920    -0.916 0.0511  0.0504     -1.01    -0.844  1.00     900.\n3 beta[2]      1.15      1.15  0.00932 0.00900     1.14     1.17   1.00     934.\n4 sigma        1.39      1.39  0.00990 0.00985     1.37     1.40   1.00    1162.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n### Quantreg\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(summary(out_freq, se = \"iid\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: quantreg::rq(formula = y ~ x, tau = q)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value     Std. Error t value   Pr(>|t|) \n(Intercept)  -0.99120   0.02167  -45.74656   0.00000\nx             1.16937   0.00374  312.48513   0.00000\n```\n\n\n:::\n:::\n\n\n### Asymmetric Laplace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_asym_laplace$summary(\"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 10\n  variable   mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  -0.997 -0.995 0.0152  0.0140  -1.02 -0.976  1.00    1193.     898.\n2 beta[2]   1.17   1.17  0.00605 0.00605  1.16  1.18   1.00    1577.    1471.\n```\n\n\n:::\n:::\n\n\nIt does appear to be better. Let's see how this does on the ImmunogG data.\n\n# Test on ImmunogG data\n\nWe'll use the same quantile of $5\\%$ to test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\nq <- 0.05\ndat <- data.frame(y = ImmunogG$IgG, \n                  alpha = 1, \n                  x = ImmunogG$Age, \n                  xsq = ImmunogG$Age^2)\n\nout_score <- score_qr$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\nout_score_mod <- score_qr_modified$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\nout_al <- asym_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\nout_freq <- rq(y ~ x + xsq, \n               data = dat,\n               tau = q)\n```\n:::\n\n\n### Results\n\n### Orginal score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 10\n  variable    mean  median     sd    mad      q5      q95  rhat ess_bulk\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -89.5   -89.2   1.28   1.15   -91.8   -87.9     1.00     555.\n2 beta[1]    0.678   0.708 0.317  0.341    0.121   1.15    1.01     262.\n3 beta[2]    1.20    1.20  0.266  0.267    0.749   1.63    1.01     268.\n4 beta[3]   -0.136  -0.136 0.0433 0.0429  -0.206  -0.0662  1.01     287.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n### Modified score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_mod$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 10\n  variable    mean  median     sd    mad      q5      q95  rhat ess_bulk\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -72.6   -72.2   1.47   1.32   -75.4   -70.8     1.01     557.\n2 beta[1]    0.676   0.686 0.239  0.259    0.260   1.03    1.01     511.\n3 beta[2]    1.21    1.22  0.184  0.188    0.921   1.53    1.01     500.\n4 beta[3]   -0.138  -0.138 0.0291 0.0293  -0.186  -0.0920  1.00     546.\n5 sigma      0.773   0.771 0.0321 0.0322   0.723   0.825   1.01     738.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\n### Quantreg:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(out_freq, se = \"iid\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: rq(formula = y ~ x + xsq, tau = q, data = dat)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  0.76514  0.30477    2.51058  0.01259\nx            1.19143  0.24792    4.80577  0.00000\nxsq         -0.13371  0.04037   -3.31187  0.00104\n```\n\n\n:::\n:::\n\n\n### Asymmetric Laplace:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_al$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 10\n  variable     mean   median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__     -671.    -670.    1.54   1.28   -673.    -669.      1.00     606.\n2 beta[1]     0.599    0.617 0.255  0.278     0.152    0.985   1.00     380.\n3 beta[2]     1.31     1.30  0.217  0.207     0.970    1.67    1.01     360.\n4 beta[3]    -0.155   -0.153 0.0369 0.0329   -0.217   -0.0993  1.01     391.\n5 sigma       0.165    0.164 0.0100 0.0100    0.150    0.182   1.01     738.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n:::\n\n\nInterestingly, the results of the score method lie between the frequentist and asymmetric Laplace methods.\n\n# Conclusion\n\nThe score method is promising but it doesn't seem like one would use the asymmetric Laplace due to the ad-hoc nature of the intercept. One thing the score method has is incorporating multipled quantiles in a principled way that allows you to put priors on the estimates that correspond to linking them. It also doesn't need to re-run the data through each quantile. In the next post, part III, I'll show this. If you have a bunch of different quantiles, lots of data, and want to understand the correlations across parameters then the score method may be right for you.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
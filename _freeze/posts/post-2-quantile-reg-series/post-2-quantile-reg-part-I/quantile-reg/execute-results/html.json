{
  "hash": "bb10160a6f229a8b1f99f6d580a868f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Quantile Regressions in Stan: Part I\"\nauthor: \"Sean Pinkney\"\ndate: \"10/15/2022\"\ncategories: [stan, quantile]\nbibliography: references.bib\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n\n::: {.cell}\n\n:::\n\n\nQuantile regressions are relatively new in the Bayesian literature first appearing in @yu who described an asymmetric Laplace distribution to estimate the conditional quantiles. This post is going to show the asymmetric Laplace and the augmented version. In part II I will show the score likelihood method.\n\nThe asymmetric Laplace distribution is given as\n\n$$\n\\operatorname{ALD}(Y \\mid x, \\beta) = \\frac{\\tau^n (1 - \\tau)^n}{\\sigma^n}\\exp \\bigg( -\\frac{\\sum_{i=1}^n \\rho_\\tau (y_i - x_i^{\\top}\\beta)}{\\sigma} \\bigg)\n$$\n\nwhere\n\n$$\n\\rho_\\tau(u) = \\frac{|u| + (2\\tau - 1)u}{2}\n$$\n\nWe may write this in Stan as\n\n\n::: {.cell output.var='asymetric_laplace'}\n\n```{.stan .cell-code}\nfunctions{\nreal q_loss(real q, vector u){\n  return 0.5 * sum(abs(u) + (2 * q - 1) * u);\n}\n\nreal ald_lpdf(vector y, real q, real sigma, vector q_est){\n  int N = num_elements(y);\n  \n  return N * (log(q) + log1m(q) - log(sigma)) - q_loss(q, y - q_est) / sigma;\n}\n}\ndata {\n  int N;                   // Number of observation\n  int P;                   // Number of predictors\n  real<lower=0, upper=1> q;\n  vector[N] y;             // Response variable sorted\n  matrix[N, P] x;\n}\nparameters {\n  vector[P] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  beta ~ normal(0, 4);\n  sigma ~ exponential(1);\n  y ~ ald(q, sigma, x * beta);\n}\n```\n:::\n\n\nLet's compare the 5th percentile to the `Brq` R package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\nout_asym_laplace <- asymetric_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(data.frame(alpha = 1, x = ImmunogG$Age, xsq = ImmunogG$Age^2))),\n  seed = 12123123,\n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n  refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\nout_asym_laplace$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 10\n  variable     mean   median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__     -671.    -670.    1.54   1.28   -673.    -669.      1.00     606.\n2 beta[1]     0.599    0.617 0.255  0.278     0.152    0.985   1.00     380.\n3 beta[2]     1.31     1.30  0.217  0.207     0.970    1.67    1.01     360.\n4 beta[3]    -0.155   -0.153 0.0369 0.0329   -0.217   -0.0993  1.01     391.\n5 sigma       0.165    0.164 0.0100 0.0100    0.150    0.182   1.01     738.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\ny = ImmunogG$IgG\nx = ImmunogG$Age\nX=cbind(x, x^2)\nfit = Brq(y ~ X , tau= 0.05,runs= 2000, burn=1000)\nprint(summary(fit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nBrq.formula(formula = y ~ X, tau = 0.05, runs = 2000, burn = 1000)\n\ntau:[1] 0.05\n\n            Estimate L.CredIntv  U.CredIntv\nIntercept  0.6201064  0.1113146  1.03475824\nXx         1.2991424  0.8962901  1.68272470\nX         -0.1550870 -0.2317421 -0.08612944\n```\n\n\n:::\n:::\n\n\n### Using the covariates for extra information\n\nIn @Lee2022 they detail a Bayesian approach to the envelope quantile regression. In all honesty, I briefly skimmed the paper and gathered that by adding a regression on the covariates and using this information in the asymmetric Laplace can help identification. This isn't a recreation of their method but a quick detour to see if a simplified version of what they do increases ESS.\n\nThe idea is that we use the variability in the covariance of the regressors to help identify the coefficients on the ALD regression. I now have to separate out the intercept into $\\alpha$.\n\n\n::: {.cell output.var='asymetric_laplace_envelope'}\n\n```{.stan .cell-code}\nfunctions{\nreal q_loss(real q, vector u){\n  return 0.5 * sum(abs(u) + (2 * q - 1) * u);\n}\n\nreal ald_lpdf(vector y, real q, real sigma, vector q_est){\n  int N = num_elements(y);\n  \n  return N * (log(q) + log1m(q) - log(sigma)) - q_loss(q, y - q_est) / sigma;\n}\n}\ndata {\n  int N;                   // Number of observation\n  int P;                   // Number of predictors\n  real<lower=0, upper=1> q;\n  vector[N] y;             // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  array[N] vector[P] X;\n  \n  for (i in 1:P) {\n    X[:, i] = to_array_1d(x[:, i]);\n  }\n}\nparameters {\n  vector[P] eta;\n  real<lower=0> sigma;\n  vector[P] mu;\n  cholesky_factor_corr[P] L;\n  vector<lower=0>[P] gamma;\n  real alpha;\n}\ntransformed parameters {\n  vector[P] beta = eta .* gamma;\n}\nmodel {\n  eta ~ normal(0, 4);\n  sigma ~ exponential(1);\n  y ~ ald(q, sigma, alpha + x * beta);\n  alpha ~ normal(0, 4);\n  \n  gamma ~ exponential(1);\n  mu ~ normal(0, 4);\n\n  X ~ multi_normal_cholesky(mu, diag_pre_multiply(gamma, L));\n}\n```\n:::\n\n\nLet's fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_envelope <- asymetric_laplace_envelope$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 2,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(data.frame(x = ImmunogG$Age, xsq = ImmunogG$Age^2))),\n  seed = 12123123,\n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n  refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n\nout_envelope$summary(c(\"alpha\", \"beta\", \"sigma\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 10\n  variable   mean median      sd     mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 alpha     0.614  0.637 0.244   0.252    0.204  0.981  1.00     913.    1012.\n2 beta[1]   1.29   1.29  0.203   0.201    0.982  1.62   1.00     863.     966.\n3 beta[2]  -0.152 -0.151 0.0345  0.0329  -0.213 -0.102  1.00     965.     946.\n4 sigma     0.166  0.165 0.00976 0.00983  0.151  0.182  1.00    1211.    1075.\n```\n\n\n:::\n:::\n\n\nSlightly different estimates, higher ESS, slower to fit, but lower standard deviations! Neat.\n\n## Scale-mixture representation\n\nThe above may also be written as a mixture of exponential and normal distributions. Letting\n\n$$\n\\begin{aligned}\nz_i &\\sim \\operatorname{Exp}(1) \\\\\n\\sigma &\\sim \\operatorname{Exp}(1) \\\\\ny_i &\\sim \\mathcal{N}(x_i^{\\top}\\beta_p + \\sigma \\theta z_i,\\, \\tau \\sigma \\sqrt{z_i})\n\\end{aligned}\n$$ where\n\n$$\n\\begin{aligned}\n\\theta &= \\frac{1 - 2 q}{q (1 - q)} \\\\\n\\tau &= \\sqrt{\\frac{2}{q(1 -q)}}\n\\end{aligned}\n$$\n\nThe following Stan model implements the augmented approach.\n\n\n::: {.cell output.var='augmented_laplace'}\n\n```{.stan .cell-code}\ndata {\n  int N;                   // Number of observation\n  int P;                   // Number of predictors\n  real<lower=0, upper=1> q;\n  vector[N] y;             // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  vector[P] beta;\n  vector<lower=0>[N] z;\n  real<lower=0> sigma;\n}\nmodel {\n  beta ~ normal(0, 2);\n  sigma ~ exponential(1);\n  \n  // Data Augmentation\n  z ~ exponential(1);\n  y ~ normal(x * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n}\n```\n:::\n\n\nLet's fit the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_aug_laplace <- augmented_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(data.frame(alpha = 1, x = ImmunogG$Age, xsq = ImmunogG$Age^2))),\n  seed = 12123123,\n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n  refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: 57 of 2000 (3.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n```\n\n\n:::\n\n```{.r .cell-code}\nout_aug_laplace$summary(c(\"beta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  variable   mean median     sd    mad     q5     q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]   0.606  0.619 0.243  0.260   0.193  0.990   1.01     303.     547.\n2 beta[2]   1.30   1.30  0.206  0.198   0.968  1.62    1.01     335.     492.\n3 beta[3]  -0.154 -0.154 0.0350 0.0325 -0.211 -0.0979  1.01     390.     669.\n```\n\n\n:::\n:::\n\n\nOuch! Divergences. Although the parameter estimates look comparable. Let's inspect the pairs plots between the augmented version and the asymmetric Laplace.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is bayesplot version 1.15.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Online documentation and vignettes at mc-stan.org/bayesplot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- bayesplot theme set to bayesplot::theme_default()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n   * Does _not_ affect other ggplot2 plots\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n   * See ?bayesplot_theme_set for details on theme setting\n```\n\n\n:::\n\n```{.r .cell-code}\nsource(\"../../../R/theme_blog.R\")\nuse_blog_bayesplot()\npairs_np_style <- bayesplot::pairs_style_np(\n  div_color = blog_colors$red,\n  td_color = blog_colors$gold\n)\n\nnp_aug_laplace <- nuts_params(out_aug_laplace)\nnp_asym_laplace <- nuts_params(out_asym_laplace)\nmcmc_pairs(out_aug_laplace$draws(c(\"beta\", \"sigma\")), np = np_aug_laplace, np_style = pairs_np_style)\n```\n\n::: {.cell-output-display}\n![](quantile-reg_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmcmc_pairs(out_asym_laplace$draws(c(\"beta\", \"sigma\")), np = np_asym_laplace, np_style = pairs_np_style)\n```\n\n::: {.cell-output-display}\n![](quantile-reg_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nThe pairs plots don't appear to show the divergence originating be due to `beta` or `sigma`. It's most likely the augmentation variables `z`. As I'm up for time on this, I'd be curious to test out alternative specifications or see if anyone has encountered these issues with the augmented version and found a solution.\n\n### Try Student T\n::: callout-warning\nThe Student T won't result in the same posterior and is likely not ALD. However, it approaches a normal distribution with increasing $\\nu$. \n:::\n\nLet's use the `student_t` with $\\nu = 30$\n\n::: {.cell output.var='student_augmented_laplace'}\n\n```{.stan .cell-code}\ndata {\n  int N;                   // Number of observation\n  int P;                   // Number of predictors\n  real<lower=0, upper=1> q;\n  vector[N] y;             // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  vector[P] beta;\n  vector<lower=0>[N] z;\n  real<lower=0> sigma;\n}\nmodel {\n  beta ~ normal(0, 2);\n  sigma ~ exponential(1);\n  \n  // Data Augmentation\n  z ~ exponential(1);\n  y ~ student_t(30, x * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n}\n```\n:::\n\n\nLet's fit the same model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_student_aug_laplace <- student_augmented_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(data.frame(alpha = 1, x = ImmunogG$Age, xsq = ImmunogG$Age^2))),\n  seed = 12123123,\n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n  refresh = 0,\n  show_messages = FALSE,\n  show_exceptions = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: 1 of 2000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n```\n\n\n:::\n\n```{.r .cell-code}\nout_student_aug_laplace$summary(c(\"beta\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  variable   mean median     sd    mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]   0.604  0.647 0.229  0.214   0.178  0.927  1.01     358.     462.\n2 beta[2]   1.34   1.33  0.164  0.153   1.10   1.63   1.01     371.     733.\n3 beta[3]  -0.156 -0.154 0.0267 0.0252 -0.202 -0.116  1.01     388.     715.\n```\n\n\n:::\n:::\n\nGreat no divergences! \n\n# Conclusion\nFor the next post, we'll take a look at an approximate likelihood using the data which asymptotically approaches the true likelihood with more data.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
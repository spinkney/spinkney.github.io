{
  "hash": "5338412dcbc361b175f18640597799ff",
  "result": {
    "markdown": "---\ntitle: \"Quantile Regressions in Stan: Part III\"\nauthor: \"Sean Pinkney\"\ndate: last-modified\ncategories: [stan, quantile]\nbibliography: references.bib\ndraft: false\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n::: {.cell}\n\n:::\n\n\nThis is part III of the quantile regression series. [Part I](/posts/post-2-quantile-reg-part-I/quantile-reg.qmd) and [part II](/posts/post-2-quantile-reg-part-II/quantile-reg-part-II.qmd) show the Bayesian quantile regression using the asymmetric Laplace distribution, augmented scheme, and score. In this post I'm going to show the multiple quantile regression given in @score, which is a follow up to part II.\n\n\n# Multiple quantile regression with score\n\nThe score method includes the regressors into the objective function as $$\ns_\\tau(\\beta) = \\sum_{i=1}^{n} x_i \\psi_\\tau(y_i - x_i^{\\top}\\beta).\n$$ where $\\psi_\\tau(u) = \\tau - I(u < 0)$.\n\n@score consider the following \"working\" likelihood \n$$\n\\mathcal{L}(y \\mid x,\\, \\beta) = C \\exp\\bigg(-\\frac{1}{2n} s_\\tau(\\beta)^{\\top} W s_\\tau(\\beta)  \\bigg),\n$$\nwhere $W$ is a $p \\times p$ positive definite weight matrix, and C is a constant free of $\\beta$. The quadratic form and the given objective function implies that the quantile estimator is maximized at the typical quantile estimate.\n\nThe $W$ matrix for multiple quantiles is given as\n\n$$\nW = (Q \\otimes G )^{-1}, \\text{ where } Q = (\\min(\\tau_i, \\tau_j) - \\tau_i \\tau_j)_{ij}, \\; G = \\frac{1}{n}\\sum_{i=1}^n x_i x_i^{\\top}.\n$$ \n\nA nice property of $W$ is that it depends all on user input data so can be constructed outside of the MCMC iterations.\n\n## Score function\n\nThe following Stan model implements the multiple $\\tau$ code. \n\n\n::: {.cell output.var='multiple_score_qr'}\n\n```{.stan .fold-show .cell-code}\nfunctions{\n  matrix kronecker(matrix A, matrix B) {\n    matrix[rows(A) * rows(B), cols(A) * cols(B)] C;\n    int m = rows(A);\n    int n = cols(A);\n    int p = rows(B);\n    int q = cols(B);\n    for (i in 1:m) {\n      for (j in 1:n) {\n        int row_start = (i - 1) * p + 1;\n        int row_end = (i - 1) * p + p;\n        int col_start = (j - 1) * q + 1;\n        int col_end = (j - 1) * q + q;\n        C[row_start:row_end, col_start:col_end] = A[i, j] * B;\n      }\n    }\n   return C;\n}\n  \n  vector q_loss(real q, vector u){\n    return (abs(u) + (2 * q - 1) * u);\n }\n    \n  vector score(vector q, vector y, matrix x, array[] vector beta) {\n    int N = num_elements(y);\n    int P = num_elements(beta[ , 1]);\n    int K = num_elements(q);\n    matrix[K, P] out;\n      \n    for (k in 1:K) {\n      out[k] = transpose(transpose(x) * q_loss(q[k], y - x * to_vector(beta[ ,k])));\n    }\n      \n    return to_vector(out);\n  }\n    \n  matrix make_w (matrix x, vector q) {\n    int N = rows(x);\n    int P = cols(x);\n    int m = num_elements(q);\n    matrix[m * P, m * P] out;\n      \n    matrix[m, m] Q;\n    matrix[P, P] G = crossprod(x) ;\n    \n  //  G[1:P, 1] /= log(N);\n  //  G[1, 2:P] = transpose(G[2:P, 1]);\n      \n    for (i in 1:m) {\n      Q[i, i] = q[i] * (1 - q[i]);\n      for (j in 1:i - 1) {\n        Q[i, j] = min([q[i], q[j]]) - q[i] * q[j];\n        Q[j, i] = Q[i, j];\n      }\n    }\n  \n    return kronecker(N * inverse(G), inverse(Q));\n  }\n}\ndata {\n  int N;               // Number of observation\n  int P;               // Number of predictors\n  int K;               // Number of quantiles\n  vector[K] q;\n  vector[N] y;         \n  matrix[N, P] x;\n}\ntransformed data {\n  matrix[K * P, K * P] W = make_w(x, q);\n}\nparameters {\n  // can add ordered constraint here\n   array[P] vector[K] beta;\n   real<lower=0> sigma;\n}\nmodel {\n  vector[K * P] score_vec = score(q, y, x, beta) / sigma;\n  \n  for (i in 1:K) \n    beta[, i] ~ normal(0, 4);\n  \n  sigma ~ exponential(1);\n \n  target += -quad_form(W, score_vec) / (2 * N * K) - K * N * log(sigma);\n\n}\n```\n:::\n\n\nI'll simulate the same data as in part II:\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nset.seed(12312)\nlibrary(quantreg)\nN     <- 1000\nx     <- runif(N, max=10)\nalpha <- -1\nbeta  <- 2\ny     <- alpha + beta * x + rnorm(N, sd = .5 * x)\nq     <- c(0.05, 0.5, 0.95)\n\n# frequentist estimate\nout_freq <- quantreg::rq(y ~ x, tau = q)\n\nout_score <- multiple_score_qr$sample(\n  data = list(N = N,\n              P = 2,\n              K = length(q),\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1,  x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 1.8 seconds.\nChain 1 finished in 2.0 seconds.\nChain 3 finished in 1.9 seconds.\nChain 4 finished in 2.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 2.3 seconds.\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 10\n  variable       mean    median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>         <dbl>     <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__      -3083.    -3083.    1.81   1.73   -3087.   -3081.     1.00     746.\n2 beta[1,1]    -1.40     -1.39  0.274  0.286     -1.85    -0.969  1.00     872.\n3 beta[2,1]     1.30      1.30  0.0430 0.0437     1.23     1.37   1.00     905.\n4 beta[1,2]    -0.867    -0.864 0.124  0.124     -1.07    -0.667  1.00     971.\n5 beta[2,2]     1.97      1.97  0.0206 0.0196     1.94     2.00   1.00    1000.\n6 beta[1,3]    -0.905    -0.936 0.218  0.207     -1.21    -0.503  1.00     971.\n7 beta[2,3]     2.83      2.84  0.0456 0.0447     2.75     2.90   1.01     944.\n8 sigma         1.69      1.69  0.0229 0.0239     1.66     1.73   1.00    1558.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n# Test on ImmunogG data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\ndat <- data.frame(y = ImmunogG$IgG, \n                  alpha = 1, \n                  x = ImmunogG$Age, \n                  xsq = ImmunogG$Age^2)\n\nout_score_mod <- multiple_score_qr$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              K = 3,\n              q = c(0.05, 0.5, 0.95),\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  max_treedepth = 12,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 5.8 seconds.\nChain 2 finished in 6.3 seconds.\nChain 4 finished in 6.9 seconds.\nChain 3 finished in 7.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 6.5 seconds.\nTotal execution time: 7.2 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_freq_1 <- rq(y ~ x + xsq, \n               data = dat,\n               tau = 0.05)\nout_freq_2 <- rq(y ~ x + xsq, \n               data = dat,\n               tau = 0.5)\nout_freq_3 <- rq(y ~ x + xsq, \n               data = dat,\n               tau = 0.95)\n```\n:::\n\n\n## Results\n\n### Modified score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_mod$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 10\n   variable       mean    median     sd    mad       q5       q95  rhat ess_bulk\n   <chr>         <dbl>     <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n 1 lp__      -605.     -605.     2.25   2.11   -609.    -602.      1.00     749.\n 2 beta[1,1]    0.648     0.682  0.443  0.441    -0.116    1.33    1.00     770.\n 3 beta[2,1]    1.20      1.20   0.387  0.376     0.545    1.85    1.01     695.\n 4 beta[3,1]   -0.138    -0.139  0.0648 0.0620   -0.247   -0.0296  1.01     727.\n 5 beta[1,2]    2.69      2.68   0.316  0.319     2.18     3.22    1.01     677.\n 6 beta[2,2]    1.20      1.21   0.238  0.235     0.810    1.58    1.01     652.\n 7 beta[3,2]   -0.0773   -0.0784 0.0374 0.0374   -0.137   -0.0137  1.01     703.\n 8 beta[1,3]    7.12      7.12   0.609  0.620     6.16     8.18    1.00     791.\n 9 beta[2,3]   -0.385    -0.393  0.494  0.505    -1.19     0.416   1.01     707.\n10 beta[3,3]    0.237     0.239  0.0788 0.0785    0.105    0.366   1.01     705.\n11 sigma        1.19      1.19   0.0270 0.0278    1.15     1.23    1.00    1373.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nQuantreg\n\n### Quantreg:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(out_freq_1, se = \"iid\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rq(formula = y ~ x + xsq, tau = 0.05, data = dat)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  0.76514  0.30477    2.51058  0.01259\nx            1.19143  0.24792    4.80577  0.00000\nxsq         -0.13371  0.04037   -3.31187  0.00104\n```\n:::\n\n```{.r .cell-code}\nsummary(out_freq_2, se = \"iid\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rq(formula = y ~ x + xsq, tau = 0.5, data = dat)\n\ntau: [1] 0.5\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  2.80112  0.59354    4.71936  0.00000\nx            1.15865  0.48282    2.39976  0.01703\nxsq         -0.07523  0.07863   -0.95677  0.33947\n```\n:::\n\n```{.r .cell-code}\nsummary(out_freq_3, se = \"iid\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rq(formula = y ~ x + xsq, tau = 0.95, data = dat)\n\ntau: [1] 0.95\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  7.23563  0.67142   10.77659  0.00000\nx           -0.48165  0.54617   -0.88186  0.37857\nxsq          0.24602  0.08895    2.76597  0.00603\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
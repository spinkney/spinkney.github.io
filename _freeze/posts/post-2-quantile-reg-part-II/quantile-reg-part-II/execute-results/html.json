{
  "hash": "53cb6e0aeeffdc0e08aeaaaf0389449f",
  "result": {
    "markdown": "---\ntitle: \"Quantile Regressions in Stan: Part II\"\nauthor: \"Sean Pinkney\"\ndate: \"10/18/2022\"\ncategories: [stan, quantile]\nbibliography: references.bib\ndraft: false\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n::: {.cell}\n\n:::\n\n\nIn [part I](/posts/post-2-quantile-reg-part-I/quantile-reg.qmd) I showed the basic Bayesian quantile regression using the asymmetric Laplace distribution and augmented scheme. In this post I'm going to show an approximate likelihood, score method proposed in @score.\n\n# First, why not use the asymmetric Laplace?\n\nThe ALD (asymmetric Laplace distribution) is great. It's fast and gives point results comparable to the frequentist estimator. However, @yang2016 showed that the asymptotic variance of the posterior distribution is \"incorrect\". It does not correspond to the frequentist interval. A reason is that the ALD and pinball loss function, $\\rho_\\tau(u) = \\frac{|u| + (2\\tau - 1)u}{2}$, is maximized at the standard frequentist quantile but does not correspond to a true data generating likelihood. In the aforementioned paper, they propose a correction to the interval after sampling. I might show this in another post but post-processing is annoying. The score method asymptotically approaches the correct estimates with the correct interval.\n\n# Quantile regression with score\n\nThe score method includes the regressors into the objective function as $$\ns_\\tau(\\beta) = \\sum_{i=1}^{n} x_i \\psi_\\tau(y_i - x_i^{\\top}\\beta).\n$$ where $\\psi_\\tau(u) = \\tau - I(u < 0)$.\n\n::: callout-warning\nWarning! It seems that $\\psi$ doesn't work for me! What does work is using $\\rho_\\tau = \\frac{|u| + (2\\tau - 1)u}{2}$ as before. They mention that the loss is the same but then use new notation indicating that it's different. Anyway, this may be what they intended.\n:::\n\n@score consider the following \"working\" likelihood $$\n\\mathcal{L}(y \\mid x,\\, \\beta) = C \\exp\\bigg(-\\frac{1}{2n} s_\\tau(\\beta)^{\\top} W s_\\tau(\\beta)  \\bigg),\n$$ where $W$ is a $p \\times p$ positive definite weight matrix, and C is a constant free of $\\beta$. The quadratic form and the given objective function implies that the quantile estimator is maximized at the typical quantile estimate.\n\nThey then choose a weight matrix which gives the correct posterior covariance as\n\n$$\nW = \\frac{n}{\\tau (1 - \\tau)}\\bigg(\\sum_{i=1}^n x_i x_i^{\\top} \\bigg).\n$$ In fact, they also show how this can incorporate multiple $\\tau$'s (I'll show this in part III) which allows more flexible modeling options, such as a squared exponential kernel that shares information across $\\beta$ quantile estimates which are near each other. The $W$ matrix for multiple quantiles is given as\n\n$$\nW = (Q \\otimes G )^{-1}, \\text{ where } Q = (\\min(\\tau_i, \\tau_j) - \\tau_i \\tau_j)_{ij}, \\; G = \\frac{1}{n}\\sum_{i=1}^n x_i x_i^{\\top}.\n$$ A nice property of $W$ is that it depends all on user input data so can be constructed outside of the MCMC iterations.\n\n## Score function\n\nThe following Stan model implements the single $\\tau$ code. I'll simulate some data and compare to the asymmetric Laplace:\n\n\n::: {.cell output.var='score_qr'}\n\n```{.stan .fold-show .cell-code}\nfunctions{\n  vector q_loss(real q, vector u){\n    return 0.5 * (abs(u) + (2 * q - 1) * u);\n  }\n    \n  vector score(real q, vector y, matrix x, vector beta) {\n    return transpose(x) * q_loss(q, y - x * beta);\n  }\n    \n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_out = inverse_spd(crossprod(x));\n\n    return alpha * x_out * N;\n  }\n}\ndata {\n  int N;                 // Number of observation\n  int P;                 // Number of predictors\n  real q;\n  vector[N] y;           // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  matrix[P, P] W = make_w(x, q);\n}\nparameters {\n  vector[P] beta;\n}\nmodel {\n  vector[P] score_vec = score(q, y, x, beta);\n\n  beta ~ normal(0, 4);\n\n  target += -quad_form(W, score_vec) / (2 * N);\n}\n```\n:::\n\n::: {.cell output.var='asym_laplace'}\n\n:::\n\n\nLet's simulate some data and see how well it performs.\n\n::: callout-note\nI have to dial up the iterations a bit for the score method as it's less efficient from an ESS perspective.\n:::\n\n\n::: {.cell}\n\n```{.r .foldable .cell-code}\nset.seed(12312)\nlibrary(quantreg)\nN     <- 1000\nx     <- runif(N, max=10)\nalpha <- -1\nbeta  <- 2\ny     <- alpha + beta * x + rnorm(N, sd = .5 * x)\nq     <- 0.05\n\n# frequentist estimate\nout_freq <- quantreg::rq(y ~ x, tau = q)\n\nout_score <- score_qr$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.6 seconds.\nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .foldable .cell-code}\nout_asym_laplace <- asym_laplace$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.3 seconds.\n```\n:::\n\n```{.r .foldable .cell-code}\nprint(summary(out_freq, se = \"boot\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: quantreg::rq(formula = y ~ x, tau = q)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept) -1.03935  0.11713   -8.87352  0.00000\nx            1.23539  0.03639   33.95156  0.00000\n```\n:::\n\n```{.r .foldable .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -832.   -831.   0.987  0.640  -834.   -831.    1.00    1172.     892.\n2 beta[1]    -1.41   -1.41 0.169  0.173    -1.68   -1.14  1.00     678.     600.\n3 beta[2]     1.30    1.30 0.0266 0.0278    1.26    1.34  1.00     712.     641.\n```\n:::\n\n```{.r .foldable .cell-code}\nout_asym_laplace$summary(\"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 10\n  variable  mean median     sd    mad    q5    q95  rhat ess_bulk ess_tail\n  <chr>    <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  -1.07  -1.06 0.0811 0.0816 -1.21 -0.957  1.01     777.     862.\n2 beta[2]   1.24   1.24 0.0181 0.0181  1.21  1.27   1.00     790.     809.\n```\n:::\n:::\n\n\n### Modified Score\n\nWe see that the frequentist estimate from `quantreg` and the asymmetric Laplace outputs are similar. The score method under-estimates the intercept where the credible interval doesn't even include -1 and the coefficient for $x$ seems to be over-estimated. Noticing this behavior, let's see what modifying the weight matrix for the intercept does. We'd like the estimate to correspond to shrinking standard deviations with increasing $N$. Something like $\\log(N)$ will work. The modification being\n\n``` stan\n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_out = crossprod(x);\n\n    x_out[2:P, 1] /= 1 + log(N);\n    x_out[1, 2:P] = transpose(x_out[2:P, 1]);\n  \n    return alpha * inverse_spd(x_out) * N;\n  }\n```\n\nLet's re-estimate the above with the modification.\n\n\n::: {.cell output.var='score_qr_modified'}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_modified <- score_qr_modified$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.5 seconds.\nChain 2 finished in 0.5 seconds.\nChain 3 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.6 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_score_modified$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -1.31e3 -1.31e3 0.889  0.667  -1.32e3 -1.31e3  1.00    1530.    2062.\n2 beta[1]  -1.15e0 -1.15e0 0.0901 0.0913 -1.30e0 -1.00e0  1.00     853.     897.\n3 beta[2]   1.26e0  1.26e0 0.0158 0.0161  1.23e0  1.29e0  1.00     869.     909.\n```\n:::\n:::\n\n\nWe now have estimates which line up a bit closer with the frequentist estimate but is completely unprincipled. Out of curiosity let's see how this compares on simulated data with $N = 10000$\n\n\n::: {.cell}\n\n:::\n\n\n### Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  <chr>        <dbl>     <dbl>   <dbl>   <dbl>     <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -9621.    -9620.    1.02    0.801   -9623.    -9.62e+3  1.00    1487.\n2 beta[1]     -0.914    -0.912 0.0357  0.0354     -0.977 -8.60e-1  1.00    1168.\n3 beta[2]      1.15      1.15  0.00668 0.00658     1.14   1.16e+0  1.00    1070.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\n### Modified Score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_modified$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable       mean     median      sd     mad      q5      q95  rhat ess_bulk\n  <chr>         <dbl>      <dbl>   <dbl>   <dbl>   <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -15351.    -15350.    1.03    0.741   -1.54e4 -1.53e+4  1.00    1657.\n2 beta[1]      -0.989     -0.989 0.0111  0.00991 -1.01e0 -9.72e-1  1.00    1512.\n3 beta[2]       1.17       1.17  0.00396 0.00403  1.16e0  1.17e+0  1.00    1711.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\n### Quantreg\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(summary(out_freq, se = \"boot\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: quantreg::rq(formula = y ~ x, tau = q)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value      Std. Error t value    Pr(>|t|)  \n(Intercept)   -0.99120    0.00865 -114.64031    0.00000\nx              1.16937    0.01477   79.18662    0.00000\n```\n:::\n:::\n\n\n### Asymmetric Laplace\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_asym_laplace$summary(\"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 10\n  variable   mean median      sd     mad    q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  -0.997 -0.995 0.0147  0.0138  -1.02 -0.974  1.00    1332.    1112.\n2 beta[2]   1.17   1.17  0.00593 0.00581  1.16  1.18   1.00    1273.    1336.\n```\n:::\n:::\n\n\nIt does appear to be better. Let's see how this does on the ImmunogG data.\n\n# Test on ImmunogG data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\ndat <- data.frame(y = ImmunogG$IgG, \n                  alpha = 1, \n                  x = ImmunogG$Age, \n                  xsq = ImmunogG$Age^2)\n\nout_score <- score_qr$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\nChain 1 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_score_mod <- score_qr_modified$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.8 seconds.\nTotal execution time: 0.9 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_al <- asym_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = q,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 0.4 seconds.\nChain 1 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\nChain 2 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_freq <- rq(y ~ x + xsq, \n               data = dat,\n               tau = q)\n```\n:::\n\n\n### Results\n\n### Orginal score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 10\n  variable    mean  median     sd    mad       q5      q95  rhat ess_bulk\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -89.5   -89.2   1.33   1.12   -92.2    -88.0     1.00     902.\n2 beta[1]    0.638   0.670 0.327  0.343    0.0655   1.14    1.00     609.\n3 beta[2]    1.23    1.23  0.276  0.272    0.776    1.68    1.00     556.\n4 beta[3]   -0.141  -0.141 0.0444 0.0433  -0.213   -0.0688  1.01     587.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\n### Modified score:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_mod$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__     -146.    -146.    1.29   1.07   -149.    -145.      1.00    1065.\n2 beta[1]     0.677    0.702 0.251  0.255     0.238    1.05    1.01     572.\n3 beta[2]     1.24     1.23  0.195  0.191     0.937    1.57    1.01     428.\n4 beta[3]    -0.144   -0.142 0.0321 0.0310   -0.199   -0.0941  1.01     455.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\n### Quantreg:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(out_freq, se = \"boot\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rq(formula = y ~ x + xsq, tau = q, data = dat)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  0.76514  0.44303    1.72707  0.08520\nx            1.19143  0.49945    2.38550  0.01769\nxsq         -0.13371  0.08580   -1.55837  0.12022\n```\n:::\n:::\n\n\n### Asymmetric Laplace:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_al$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 10\n  variable     mean   median      sd     mad       q5      q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>   <dbl>   <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -670.    -670.    1.50    1.24    -673.    -669.     1.01     585.\n2 beta[1]     0.587    0.601 0.246   0.271      0.179    0.962  1.02     385.\n3 beta[2]     1.31     1.31  0.199   0.210      1.01     1.64   1.02     336.\n4 beta[3]    -0.156   -0.155 0.0338  0.0333    -0.212   -0.104  1.01     373.\n5 sigma       0.165    0.165 0.00958 0.00897    0.149    0.181  1.01     645.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nInterestingly, the results of the score method lie between the frequentist and asymmetric Laplace methods.\n\n# Conclusion\n\nThe score method is promising but it doesn't seem like one would use the asymmetric Laplace due to the ad-hoc nature of the intercept. One thing the score method has is incorporating multipled quantiles in a principled way that allows you to put priors on the estimates that correspond to linking them. It also doesn't need to re-run the data through each quantile. In the next post, part III, I'll show this. If you have a bunch of different quantiles, lots of data, and want to understand the correlations across parameters then the score method may be right for you.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
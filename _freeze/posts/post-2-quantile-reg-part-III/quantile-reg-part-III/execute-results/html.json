{
  "hash": "84cf937a21d60c8e198b607d126860d5",
  "result": {
    "markdown": "---\ntitle: \"Quantile Regressions in Stan: Part III\"\nauthor: \"Sean Pinkney\"\ndate: last-modified\ncategories: [stan, quantile]\nbibliography: references.bib\ndraft: true\nformat:\n  html:\n    code-fold: true\n    code-summary: \"Show the code\"\n---\n\n::: {.cell}\n\n:::\n\n\nIn [part I](/posts/post-2-quantile-reg-part-I/quantile-reg.qmd) I showed the basic Bayesian quantile regression using the asymmetric Laplace distribution and augmented scheme. In this post I'm going to show an approximate likelihood, score method proposed in @score.\n\n# First, why not use the asymmetric Laplace?\n\nThe ALD (asymmetric Laplace distribution) is great. It's fast and gives point results comparable to the frequentist estimator. However, @yang2016 showed that the asymptotic variance of the posterior distribution is \"incorrect\". It does not correspond to the frequentist interval. A reason is that the ALD and pinball loss function, $\\rho_\\tau(u) = \\frac{|u| + (2\\tau - 1)u}{2}$, is maximized at the standard frequentist quantile but does not correspond to a true data generating likelihood. In the aforementioned paper, they propose a correction to the interval after sampling. I might show this in another post but post-processing is annoying. The score method asymptotically approaches the correct estimates with the correct interval.\n\n# Quantile regression with score\n\nThe score method includes the regressors into the objective function as $$\ns_\\tau(\\beta) = \\sum_{i=1}^{n} x_i \\psi_\\tau(y_i - x_i^{\\top}\\beta).\n$$ where $\\psi_\\tau(u) = \\tau - I(u < 0)$.\n\n::: callout-warning\nWarning! It seems that $\\psi$ doesn't work for me! What does work is using $\\rho_\\tau = \\frac{|u| + (2\\tau - 1)u}{2}$ as before. They mention that the loss is the same but then use new notation indicating that it's different. Anyway, this may be what they intended.\n:::\n\n@score consider the following \"working\" likelihood $$\n\\mathcal{L}(y \\mid x,\\, \\beta) = C \\exp\\bigg(-\\frac{1}{2n} s_\\tau(\\beta)^{\\top} W s_\\tau(\\beta)  \\bigg),\n$$ where $W$ is a $p \\times p$ positive definite weight matrix, and C is a constant free of $\\beta$. The quadratic form and the given objective function implies that the quantile estimator is maximized at the typical quantile estimate.\n\nThey then choose a weight matrix which gives the correct posterior covariance as\n\n$$\nW = \\frac{n}{\\tau (1 - \\tau)}\\bigg(\\sum_{i=1}^n x_i x_i^{\\top} \\bigg).\n$$ In fact, they also show how this can incorporate multiple $\\tau$'s (I'll show this in part III) which allows more flexible modeling options, such as a squared exponential kernel that shares information across $\\beta$ quantile estimates which are near each other. The $W$ matrix for multiple quantiles is given as\n\n$$\nW = (Q \\otimes G )^{-1}, \\text{ where } Q = (\\min(\\tau_i, \\tau_j) - \\tau_i \\tau_j)_{ij}, \\; G = \\frac{1}{n}\\sum_{i=1}^n x_i x_i^{\\top}.\n$$ A nice property of $W$ is that it depends all on user input data so can be constructed outside of the MCMC iterations.\n\n## Score function\n\nThe following Stan model implements the single $\\tau$ code. I'll simulate some data and compare to the asymmetric Laplace:\n\n\n::: {.cell output.var='score_qr'}\n\n```{.stan .fold-show .cell-code}\nfunctions{\n  vector q_loss(real q, vector u){\n    return 0.5 * (abs(u) + (2 * q - 1) * u);\n  }\n    \n  vector score(real q, vector y, matrix x, vector beta) {\n    return transpose(x) * q_loss(q, y - x * beta);\n  }\n    \n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_out = inverse_spd(crossprod(x));\n\n    return alpha * x_out * N;\n  }\n}\ndata {\n  int N;                 // Number of observation\n  int P;                 // Number of predictors\n  real q;\n  vector[N] y;           // Response variable sorted\n  matrix[N, P] x;\n}\ntransformed data {\n  matrix[P, P] W = make_w(x, q);\n}\nparameters {\n  vector[P] beta;\n}\nmodel {\n  vector[P] score_vec = score(q, y, x, beta);\n\n  beta ~ normal(0, 4);\n\n  target += -quad_form(W, score_vec) / (2 * N);\n}\n```\n:::\n\n::: {.cell output.var='asym_laplace'}\n\n:::\n\n\nLet's simulate some data and see how well it performs.\n\n::: callout-note\nI have to dial up the iterations a bit for the score method as it's less efficient from an ESS perspective.\n:::\n\n\n::: {.cell}\n\n```{.r .foldable .cell-code}\nset.seed(12312)\nlibrary(quantreg)\nN     <- 1000\nx     <- runif(N, max=10)\nalpha <- -1\nbeta  <- 2\ny     <- alpha + beta * x + rnorm(N, sd = .5 * x)\nq     <- 0.05\n\n# frequentist estimate\nout_freq <- quantreg::rq(y ~ x, tau = q)\n\nout_score <- score_qr$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\nChain 2 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .foldable .cell-code}\nout_asym_laplace <- asym_laplace$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.4 seconds.\n```\n:::\n\n```{.r .foldable .cell-code}\nprint(summary(out_freq, se = \"boot\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: quantreg::rq(formula = y ~ x, tau = q)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept) -1.03935  0.11713   -8.87352  0.00000\nx            1.23539  0.03639   33.95156  0.00000\n```\n:::\n\n```{.r .foldable .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 lp__     -832.   -831.   0.987  0.640  -834.   -831.    1.00    1172.     892.\n2 beta[1]    -1.41   -1.41 0.169  0.173    -1.68   -1.14  1.00     678.     600.\n3 beta[2]     1.30    1.30 0.0266 0.0278    1.26    1.34  1.00     712.     641.\n```\n:::\n\n```{.r .foldable .cell-code}\nout_asym_laplace$summary(\"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 10\n  variable  mean median     sd    mad    q5    q95  rhat ess_bulk ess_tail\n  <chr>    <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  -1.07  -1.06 0.0811 0.0816 -1.21 -0.957  1.01     777.     862.\n2 beta[2]   1.24   1.24 0.0181 0.0181  1.21  1.27   1.00     790.     809.\n```\n:::\n:::\n\n\n### Modified Score\n\nWe see that the frequentist estimate from `quantreg` and the asymmetric Laplace outputs are similar. The score method under-estimates the intercept where the credible interval doesn't even include -1 and the coefficient for $x$ seems to be over-estimated. Noticing this behavior, I found that modifying the weight matrix to put 0s in for the intercept and setting the first element to $1 + (P - 1) \\alpha^{P - 1} = 1 + (P -1)[q (1 - q)]^{P-1}$ provides better estimates. I have no theoretical justification for this, only that it will equal 1 when only the intercept is estimated and the standard errors and estimates are closer to the asymmetric Laplace when providing this modification.\n\n\n::: {.cell output.var='make_w'}\n\n```{.stan .foldable .cell-code}\n  matrix make_w (matrix x, real q) {\n    int N = rows(x);\n    int P = cols(x);\n    real alpha = 1 / (q * (1 - q));\n    matrix[P, P] x_reg = crossprod(x);\n    matrix[P, P] x_out;\n    \n    x_out[2:P, 2:P] = x_reg[2:P, 2:P];\n    x_out[1:P, 1] = rep_vector(0, P);\n    x_out[1, 1:P] = transpose(x_out[1:P, 1]);\n    x_out[1, 1] =  1 + (P - 1) * alpha^(P - 1);\n     \n    return alpha * inverse_spd(x_out) * N;\n  }\n```\n:::\n\n::: {.cell output.var='score_qr_modified'}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nout_score <- score_qr_modified$sample(\n  data = list(N = N,\n              P = 2,\n              q = q,\n              y = y,\n              x = as.matrix(data.frame(alpha = 1, x = x))),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.7 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.8 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_score$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  <chr>        <dbl>     <dbl>   <dbl>   <dbl>     <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -29994.   -29994.   0.856   0.593   -29995.   -3.00e+4  1.00    1190.\n2 beta[1]      -1.02     -1.02 0.0164  0.0176      -1.04 -9.92e-1  1.00     499.\n3 beta[2]       1.23      1.23 0.00292 0.00304      1.23  1.24e+0  1.00     555.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nNow we get estimates which are closer to what we get with the asymmetric Laplace!\n\n# Test on ImmunogG data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\ndat <- data.frame(y = ImmunogG$IgG, \n                  alpha = 1, \n                  x = ImmunogG$Age, \n                  xsq = ImmunogG$Age^2)\n\nout_score_mod <- score_qr_modified$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 1000,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.8 seconds.\nChain 2 finished in 0.8 seconds.\nChain 3 finished in 0.8 seconds.\nChain 4 finished in 0.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.8 seconds.\nTotal execution time: 0.9 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_al <- asym_laplace$sample(\n  data = list(N = nrow(ImmunogG),\n              P = 3,\n              q = 0.05,\n              y = ImmunogG$IgG,\n              x = as.matrix(dat[, 2:4])),\n  seed = 12123123, \n  parallel_chains = 4,\n  iter_warmup = 500,\n  iter_sampling = 500,\n    refresh = 0,\n  show_messages = FALSE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 0.4 seconds.\nChain 1 finished in 0.5 seconds.\nChain 4 finished in 0.5 seconds.\nChain 2 finished in 0.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.5 seconds.\nTotal execution time: 0.7 seconds.\n```\n:::\n\n```{.r .cell-code}\nout_freq <- rq(y ~ x + xsq, \n               data = dat,\n               tau = 0.05)\n```\n:::\n\n\n## Results\n\n### Modified score:\n \n\n::: {.cell}\n\n```{.r .cell-code}\nout_score_mod$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 10\n  variable     mean   median     sd    mad       q5       q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>  <dbl>  <dbl>    <dbl>     <dbl> <dbl>    <dbl>\n1 lp__     -114.    -114.    1.28   1.11   -117.    -113.      1.01     702.\n2 beta[1]     0.697    0.724 0.311  0.316     0.148    1.16    1.01     565.\n3 beta[2]     1.22     1.22  0.246  0.242     0.829    1.63    1.01     480.\n4 beta[3]    -0.144   -0.143 0.0402 0.0384   -0.208   -0.0813  1.01     504.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nQuantreg\n\n### Quantreg:\n \n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(out_freq, se = \"boot\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall: rq(formula = y ~ x + xsq, tau = 0.05, data = dat)\n\ntau: [1] 0.05\n\nCoefficients:\n            Value    Std. Error t value  Pr(>|t|)\n(Intercept)  0.76514  0.40466    1.89082  0.05963\nx            1.19143  0.48030    2.48058  0.01367\nxsq         -0.13371  0.08514   -1.57043  0.11739\n```\n:::\n:::\n\n\n### Asymmetric Laplace:\n \n\n::: {.cell}\n\n```{.r .cell-code}\nout_al$summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 10\n  variable     mean   median      sd     mad       q5      q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>   <dbl>   <dbl>    <dbl>    <dbl> <dbl>    <dbl>\n1 lp__     -670.    -670.    1.50    1.24    -673.    -669.     1.01     585.\n2 beta[1]     0.587    0.601 0.246   0.271      0.179    0.962  1.02     385.\n3 beta[2]     1.31     1.31  0.199   0.210      1.01     1.64   1.02     336.\n4 beta[3]    -0.156   -0.155 0.0338  0.0333    -0.212   -0.104  1.01     373.\n5 sigma       0.165    0.165 0.00958 0.00897    0.149    0.181  1.01     645.\n# … with 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nInterestingly, the results of the score method lie between the frequentist and asymmetric Laplace methods.\n\n# Conclusion\n\nThe score method is promising but it doesn't seem like one would use the asymmetric Laplace due to the ad-hoc nature of the intercept. One thing the score method has is incorporating multipled quantiles in a principled way that allows you to put priors on the estimates that correspond to linking them. It also doesn't need to re-run the data through each quantile. In the next post, part III, I'll show this. If you have a bunch of different quantiles, lots of data, and want to understand the correlations across parameters then the score method may be right for you.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}